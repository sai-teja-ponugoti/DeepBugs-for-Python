{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokens_to_vector.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4bI+CmzsglMmLPpSCQRY+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sai-teja-ponugoti/DeepBugs-for-Python/blob/master/Python/tokens_to_vector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK9kW-23oCrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tarfile\n",
        "# !wget http://files.srl.inf.ethz.ch/data/py150_files.tar.gz\n",
        "# tf = tarfile.open(\"py150_files.tar.gz\")\n",
        "# tf.extractall()\n",
        "# tf = tarfile.open(\"data.tar.gz\")\n",
        "# tf.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypd3j9iGn7Wf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "cafe7319-62ac-4e3d-e79e-a9d363d5be39"
      },
      "source": [
        "# This program adds two numbers\n",
        "import re\n",
        "import os\n",
        "from io import BytesIO\n",
        "import tokenize\n",
        "from tokenize import tok_name\n",
        "import keyword\n",
        "\n",
        "all_token_text_list = []\n",
        "python_key_words = []\n",
        "filepath = 'python100k_train.txt'\n",
        "count = 0\n",
        "with open(filepath) as fp:\n",
        "    line = fp.readline()\n",
        "    while line:\n",
        "        # print(line)\n",
        "        count += 1\n",
        "        # if count>5:\n",
        "            # break\n",
        "            # print(\"finished \",count)\n",
        "        if count%10000 == 0:\n",
        "            print(\"finished \",count)\n",
        "\n",
        "        string_of_tokens = []\n",
        "        list_of_tokens = ''\n",
        "        list_of_tokens = []\n",
        "\n",
        "        # print(os.path.join(r\"./\", line.strip()))\n",
        "        # break\n",
        "        # with tokenize.open('/content/data/HenryHu/pybbs/digest.py') as f:.\n",
        "        try:\n",
        "\n",
        "            with tokenize.open(os.path.join(r\"./\", line.strip())) as f:\n",
        "                # print(f.readline)\n",
        "                # print(type(f.readline))\n",
        "                # stri = re.sub(r\"[\\t]*\", \"\", )\n",
        "                # print(\"opened files\")\n",
        "\n",
        "                tokens = tokenize.generate_tokens(f.readline)\n",
        "                # print(\"tokenized the file\")\n",
        "                for token in tokens:\n",
        "                    # print(token.start)\n",
        "                    if tok_name[token.type] == \"NAME\":\n",
        "                        if token.string not in ['True', 'False', 'Null', 'None'] and token.string not in keyword.kwlist:\n",
        "                            # list_of_tokens += \"ID:\" + token.string + \"$%#~\"\n",
        "                            list_of_tokens.append(\"ID:\" + token.string)\n",
        "                            # string_of_tokens.append()\n",
        "                        elif token.string in ['True', 'False', 'Null', 'None']:\n",
        "                            # list_of_tokens += \"LIT:\" + token.string + \"$%#~\"\n",
        "                            list_of_tokens.append(\"LIT:\" + token.string)\n",
        "                        elif token.string in keyword.kwlist:\n",
        "                            # list_of_tokens += \"STD:\" + token.string + \"$%#~\"\n",
        "                            list_of_tokens.append(\"STD:\" + token.string)\n",
        "                    elif tok_name[token.type] in [\"NUMBER\", \"STRING\"]:\n",
        "                        # list_of_tokens += \"LIT:\" + token.string + \"$%#~\"\n",
        "                        list_of_tokens.append(\"LIT:\" + token.string)\n",
        "                        \n",
        "                    else:\n",
        "                        # print(tok_name[token.type],end='')\n",
        "                        # print(token.string,end='')\n",
        "                        # print(\"done\")\n",
        "                        if token.string == '\\n':\n",
        "                            # list_of_tokens += r\"STD:\\n\" + \" \"\n",
        "                            list_of_tokens.append(r\"STD:\\n\")\n",
        "                        elif tok_name[token.type] == \"INDENT\":\n",
        "                            # list_of_tokens += r\"STD:\\t\" + \"$%#~\"\n",
        "                            list_of_tokens.append(r\"STD:\\t\")\n",
        "                        elif tok_name[token.type] == \"DEDENT\":\n",
        "                            # list_of_tokens += r\"STD:DEDENT\" + \"$%#~\"\n",
        "                            list_of_tokens.append(r\"STD:DEDENT\")\n",
        "                        elif tok_name[token.type] == \"ENDMARKER\":\n",
        "                            # list_of_tokens += r\"STD:ENDMARKER\" + \"$%#~\"\n",
        "                            list_of_tokens.append(r\"STD:ENDMARKER\")\n",
        "                        else:\n",
        "                            # list_of_tokens += \"STD:\" + token.string + \"$%#~\"\n",
        "                            list_of_tokens.append(\"STD:\" + token.string)\n",
        "        except Exception as e:\n",
        "            print(\"error occured at file \",count)\n",
        "        all_token_text_list.append(list_of_tokens)\n",
        "        line = fp.readline()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "finished  10000\n",
            "finished  20000\n",
            "finished  30000\n",
            "finished  40000\n",
            "finished  50000\n",
            "error occured at file  50521\n",
            "error occured at file  55740\n",
            "finished  60000\n",
            "finished  70000\n",
            "finished  80000\n",
            "finished  90000\n",
            "finished  100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkcNqNsQ_-8p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86fee9ca-90be-497c-ed11-270361ff6007"
      },
      "source": [
        "len(all_token_text_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP9ctB2JoU2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# all_token_text_list[0].index(\"LIT:True\")\n",
        "# all_token_text_list\n",
        "# if 'STD:=' in all_token_text_list[0] :\n",
        "    # print(\"Yes, 'at' found in List : \" , all_token_text_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B-DQUTF2g0_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "9c3aa77a-c0c7-4fcb-df78-bef94cea47ec"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import math\n",
        "import time\n",
        "\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "logging.root.level = logging.INFO\n",
        "\n",
        "nb_tokens_in_context = 20\n",
        "kept_tokens = 100000\n",
        "embedding_size = 200 \n",
        "\n",
        "model = Word2Vec(all_token_text_list,window=10, min_count=1,size=embedding_size, workers=400, iter=1)\n",
        "time_stamp = math.floor(time.time() * 1000)\n",
        "# model.save(\"embedding_model_\" + str(time_stamp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-17 03:45:36,357 : INFO : collecting all words and their counts\n",
            "2020-08-17 03:45:36,358 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2020-08-17 03:45:38,205 : INFO : PROGRESS: at sentence #10000, processed 9480249 words, keeping 499959 word types\n",
            "2020-08-17 03:45:40,123 : INFO : PROGRESS: at sentence #20000, processed 19164202 words, keeping 899515 word types\n",
            "2020-08-17 03:45:41,986 : INFO : PROGRESS: at sentence #30000, processed 28669927 words, keeping 1253255 word types\n",
            "2020-08-17 03:45:43,906 : INFO : PROGRESS: at sentence #40000, processed 38024563 words, keeping 1586247 word types\n",
            "2020-08-17 03:45:45,886 : INFO : PROGRESS: at sentence #50000, processed 48132335 words, keeping 1918703 word types\n",
            "2020-08-17 03:45:47,748 : INFO : PROGRESS: at sentence #60000, processed 57649591 words, keeping 2238247 word types\n",
            "2020-08-17 03:45:49,697 : INFO : PROGRESS: at sentence #70000, processed 67562656 words, keeping 2544456 word types\n",
            "2020-08-17 03:45:51,910 : INFO : PROGRESS: at sentence #80000, processed 77355210 words, keeping 2845526 word types\n",
            "2020-08-17 03:45:53,922 : INFO : PROGRESS: at sentence #90000, processed 87616225 words, keeping 3144326 word types\n",
            "2020-08-17 03:45:55,843 : INFO : collected 3411112 word types from a corpus of 97104968 raw words and 100000 sentences\n",
            "2020-08-17 03:45:55,844 : INFO : Loading a fresh vocabulary\n",
            "2020-08-17 03:46:20,216 : INFO : effective_min_count=1 retains 3411112 unique words (100% of original 3411112, drops 0)\n",
            "2020-08-17 03:46:20,218 : INFO : effective_min_count=1 leaves 97104968 word corpus (100% of original 97104968, drops 0)\n",
            "2020-08-17 03:46:32,393 : INFO : deleting the raw counts dictionary of 3411112 items\n",
            "2020-08-17 03:46:32,456 : INFO : sample=0.001 downsamples 23 most-common words\n",
            "2020-08-17 03:46:32,457 : INFO : downsampling leaves estimated 46479899 word corpus (47.9% of prior 97104968)\n",
            "2020-08-17 03:46:45,095 : INFO : estimated required memory for 3411112 words and 200 dimensions: 7163335200 bytes\n",
            "2020-08-17 03:46:45,096 : INFO : resetting layer weights\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHEKSg2a7vpt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "75df164a-2baf-4d32-c490-dc9990e30be5"
      },
      "source": [
        "import json\n",
        "token_to_vector = dict()\n",
        "for token in model.wv.vocab:\n",
        "        if token.startswith(\"ID:\") or token.startswith(\"LIT:\"):\n",
        "            vector = model[token].tolist()\n",
        "            token_to_vector[token] = vector\n",
        "# token_to_vector_file_name = \"token_to_vector_\" + str(time_stamp) + \".json\"\n",
        "with open(\"token_to_vector_new.json\", \"w\") as file:\n",
        "    json.dump(token_to_vector, file, sort_keys=True, indent=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsSA08_x3CY0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "4142491c-2df1-482e-f746-d516bd1cf8a6"
      },
      "source": [
        "# # model.wv\n",
        "# model.wv.similarity('ID:num1', 'ID:num2')\n",
        "# # model.wv.similarity('STD:\\\\n', 'STD:\\\\t')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.26086137"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8PpzeCz5rHQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ef39254-0834-4971-b78f-1d5d42b4904d"
      },
      "source": [
        "# model.running_training_los"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    }
  ]
}